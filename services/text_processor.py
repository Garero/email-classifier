"""
M√≥dulo de processamento de linguagem natural (NLP) para emails.
Implementa t√©cnicas avan√ßadas: tokeniza√ß√£o, remo√ß√£o de stop words,
stemming, lemmatiza√ß√£o e limpeza de texto.
"""

import re
import unicodedata
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

# Stop words em portugu√™s brasileiro (palavras comuns sem valor sem√¢ntico)
STOP_WORDS_PT = {
    'a', 'o', 'e', '√©', 'de', 'da', 'do', 'em', 'um', 'uma', 'os', 'as', 'dos', 'das',
    'para', 'com', 'por', 'sem', 'sob', 'sobre', 'ao', 'aos', '√†', '√†s', 'no', 'na',
    'nos', 'nas', 'pelo', 'pela', 'pelos', 'pelas', 'que', 'qual', 'quando', 'onde',
    'como', 'se', 'mas', 'mais', 'menos', 'muito', 'pouco', 'todo', 'toda', 'todos',
    'todas', 'outro', 'outra', 'outros', 'outras', 'mesmo', 'mesma', 'mesmos', 'mesmas',
    'tal', 'tais', 'este', 'esta', 'estes', 'estas', 'esse', 'essa', 'esses', 'essas',
    'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'isso', 'aquilo', 'eu', 'tu',
    'ele', 'ela', 'n√≥s', 'v√≥s', 'eles', 'elas', 'me', 'te', 'se', 'lhe', 'nos', 'vos',
    'lhes', 'meu', 'minha', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'seu',
    'sua', 'seus', 'suas', 'nosso', 'nossa', 'nossos', 'nossas', 'vosso', 'vossa',
    'vossos', 'vossas', 'ser', 'estar', 'ter', 'haver', 'fazer', 'ir', 'poder', 'dar',
    'ver', 'saber', 'querer', 'dizer', 'ol√°', 'oi', 'obrigado', 'obrigada', 'por favor',
    'att', 'atenciosamente', 'cordialmente', 'abs', 'abra√ßo', 'abra√ßos'
}

# Palavras-chave para classifica√ß√£o PRODUTIVA (setor financeiro)
PRODUCTIVE_KEYWORDS = {
    # Problemas e erros
    'problema', 'erro', 'bug', 'falha', 'defeito', 'avaria', 'quebra', 'pane', 'queda',
    'n√£o funciona', 'n√£o est√° funcionando', 'parou', 'travou', 'travando', 'lentid√£o', 'lento', 'congelou',
    'fora do ar', 'inoperante', 'inacess√≠vel', 'bloqueado', 'quebrado', 'cr√≠tico',
    
    # Solicita√ß√µes e pedidos
    'solicit', 'pedid', 'requer', 'requisi', 'demand', 'necessit', 'precis',
    'solicita√ß√£o', 'pedido', 'requisi√ß√£o', 'demanda',
    
    # Suporte t√©cnico
    'suporte', 'ajuda', 'assist', 'suport', 'auxil', 'socorro', 'resolut',
    'suporte t√©cnico', 'assist√™ncia t√©cnica', 'atendimento',
    
    # D√∫vidas e perguntas
    'd√∫vida', 'pergunta', 'question', 'indag', 'consult', 'esclarec', 'explic',
    'como fazer', 'como usar', 'como configurar',
    
    # Prazos e urg√™ncia
    'prazo', 'urgente', 'urg√™ncia', 'prioridade', 'priorit√°rio', 'imediato', 'imediatamente', 'r√°pido',
    'asap', 'hoje', 'amanh√£', 'data', 'vencimento', 'limite',
    
    # Financeiro espec√≠fico
    'transa√ß√£o', 'pagamento', 'cobran√ßa', 'fatura', 'boleto', 'd√©bito', 'cr√©dito',
    'extrato', 'saldo', 'conta', 'cart√£o', 'transfer√™ncia', 'ted', 'doc', 'pix',
    'investimento', 'aplica√ß√£o', 'renda', 'juros', 'taxa', 'tarifa', 'comiss√£o',
    'empr√©stimo', 'financiamento', 'parcela', 'divida', 'calote', 'inadimplente',
    'seguro', 'sinistro', 'indeniza√ß√£o', 'ap√≥lice',
    
    # Sistemas e tecnologia
    'sistema', 'aplicativo', 'app', 'software', 'hardware', 'login', 'senha',
    'acesso', 'conex√£o', 'internet', 'rede', 'servidor', 'banco de dados',
    'backup', 'restaura√ß√£o', 'atualiza√ß√£o', 'upgrade',
    
    # Configura√ß√µes
    'configurar', 'instalar', 'implementar', 'integrar', 'personalizar', 'ajust',
    'configura√ß√£o', 'instala√ß√£o', 'implementa√ß√£o',
    
    # Relat√≥rios e documentos
    'relat√≥rio', 'documento', 'contrato', 'proposta', 'or√ßamento', 'fatura',
    'nota fiscal', 'recibo', 'comprovante', 'certificado',
    
    # Reuni√µes e contatos
    'reuni√£o', 'encontro', 'confer√™ncia', 'apresenta√ß√£o', 'reuni', 'encontr',
    'visita', 'contato', 'telefone', 'email', 'whatsapp',
    
    # Status e acompanhamento
    'status', 'andamento', 'progresso', 'situa√ß√£o', 'estado', 'acompanh',
    'atualiza√ß√£o', 'novidade', 'evolu√ß√£o',
    
    # Legal e conformidade
    'legal', 'jur√≠dico', 'contrato', 'termo', 'cl√°usula', 'lei', 'norma',
    'regulamento', 'compliance', 'auditoria', 'fiscaliza√ß√£o'
}

# Palavras-chave para classifica√ß√£o IMPRODUTIVA
UNPRODUCTIVE_KEYWORDS = {
    # Agradecimentos
    'obrigado', 'obrigada', 'agrade√ßo', 'agradecimento', 'grato', 'grata',
    'valeu', 'brigado', 'brigada',
    
    # Parab√©ns e felicita√ß√µes
    'parab√©ns', 'congratulations', 'felicita√ß√µes', 'feliz', 'felicidade',
    'comemora√ß√£o', 'celebra√ß√£o',
    
    # Cumprimentos sociais
    'bom dia', 'boa tarde', 'boa noite', 'ol√°', 'oi', 'sauda√ß√µes', 'cumprimentos',
    'sauda√ß√£o', 'cumprimento', 'sauda√ß√µes', 'saudacoes',
    
    # Mensagens pessoais
    'abra√ßo', 'abra√ßos', 'beijo', 'beijos', 'carinho', 'afeto', 'amizade',
    'familia', 'fam√≠lia', 'amigo', 'amiga',
    
    # Eventos sociais
    'natal', 'ano novo', 'r√©veillon', 'pascoa', 'p√°scoa', 'carnaval', 'feriado',
    'fest', 'festa', 'confraterniza√ß√£o', 'evento social',
    
    # Mensagens autom√°ticas
    'autom√°tico', 'autom√°tica', 'auto resposta', 'auto-resposta', 'responder',
    'n√£o responda', 'do not reply',
    
    # Newsletters e marketing
    'newsletter', 'boletim', 'informativo', 'promo√ß√£o', 'promocional', 'oferta',
    'desconto', 'cupom', 'marketing', 'publicidade',
    
    # Fora do contexto profissional
    'pessoal', 'particular', 'privado', 'intimo', '√≠ntimo'
}

# Dicion√°rio de stemming simples (sufixos comuns em portugu√™s)
STEMMING_RULES = [
    ('amente', ''),      # rapidamente -> rapid
    ('mente', ''),       # felizmente -> feliz
    ('a√ß√£o', ''),        # solicita√ß√£o -> solicit
    ('√ß√µes', ''),        # solicita√ß√µes -> solicit
    ('ador', ''),        # trabalhador -> trabalh
    ('ante', ''),        # estudante -> estud
    ('√™ncia', ''),       # urg√™ncia -> urg
    ('√¢ncia', ''),       # import√¢ncia -> import
    ('ismo', ''),        # capitalismo -> capital
    ('ista', ''),        # dentista -> dent
    ('oso', ''),         # generoso -> gener
    ('osa', ''),         # generosa -> gener
    ('ivo', ''),         # produtivo -> produt
    ('iva', ''),         # produtiva -> produt
    ('mente', ''),       # certamente -> cert
    ('idade', ''),       # rapidez -> rapid
    ('ar', ''),          # trabalhar -> trabalh
    ('er', ''),          # fazer -> faz
    ('ir', ''),          # partir -> part
]


class TextProcessor:
    """Processador de texto com t√©cnicas de NLP"""
    
    def __init__(self, remove_stopwords: bool = True, apply_stemming: bool = True):
        """
        Inicializa o processador de texto.
        
        Args:
            remove_stopwords: Se True, remove stop words
            apply_stemming: Se True, aplica stemming
        """
        self.remove_stopwords = remove_stopwords
        self.apply_stemming = apply_stemming
        self.stop_words = STOP_WORDS_PT
        
    def normalize_text(self, text: str) -> str:
        """
        Normaliza texto removendo acentos e caracteres especiais.
        
        Args:
            text: Texto a ser normalizado
            
        Returns:
            Texto normalizado
        """
        # Converte para lowercase
        text = text.lower()
        
        # Remove acentos (mant√©m √ß)
        nfkd = unicodedata.normalize('NFKD', text)
        text = ''.join([c for c in nfkd if not unicodedata.combining(c)])
        
        return text
    
    def tokenize(self, text: str) -> List[str]:
        """
        Tokeniza o texto em palavras individuais.
        
        Args:
            text: Texto a ser tokenizado
            
        Returns:
            Lista de tokens (palavras)
        """
        # Remove pontua√ß√£o e caracteres especiais, mantendo apenas palavras
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Divide em tokens (palavras)
        tokens = text.split()
        
        # Remove tokens vazios
        tokens = [t for t in tokens if t.strip()]
        
        return tokens
    
    def remove_stop_words(self, tokens: List[str]) -> List[str]:
        """
        Remove stop words (palavras comuns sem valor sem√¢ntico).
        
        Args:
            tokens: Lista de tokens
            
        Returns:
            Lista de tokens sem stop words
        """
        return [t for t in tokens if t.lower() not in self.stop_words]
    
    def stem_word(self, word: str) -> str:
        """
        Aplica stemming em uma palavra (reduz √† raiz).
        
        Args:
            word: Palavra a ser processada
            
        Returns:
            Palavra com stemming aplicado
        """
        word_lower = word.lower()
        
        # Aplica regras de stemming
        for suffix, replacement in STEMMING_RULES:
            if word_lower.endswith(suffix) and len(word_lower) > len(suffix) + 2:
                return word_lower[:-len(suffix)] + replacement
        
        return word_lower
    
    def stem_tokens(self, tokens: List[str]) -> List[str]:
        """
        Aplica stemming em uma lista de tokens.
        
        Args:
            tokens: Lista de tokens
            
        Returns:
            Lista de tokens com stemming aplicado
        """
        if not self.apply_stemming:
            return tokens
        return [self.stem_word(t) for t in tokens]
    
    def extract_keywords(self, text: str, top_n: int = 15) -> List[str]:
        """
        Extrai palavras-chave mais relevantes do texto.
        
        Args:
            text: Texto para an√°lise
            top_n: N√∫mero de palavras-chave a retornar
            
        Returns:
            Lista de palavras-chave mais frequentes
        """
        # Normaliza e tokeniza
        normalized = self.normalize_text(text)
        tokens = self.tokenize(normalized)
        
        # Remove stop words
        if self.remove_stopwords:
            tokens = self.remove_stop_words(tokens)
        
        # Aplica stemming
        if self.apply_stemming:
            tokens = self.stem_tokens(tokens)
        
        # Conta frequ√™ncia
        freq = {}
        for token in tokens:
            if len(token) > 2:  # Ignora palavras muito curtas
                freq[token] = freq.get(token, 0) + 1
        
        # Ordena por frequ√™ncia
        sorted_keywords = sorted(freq.items(), key=lambda x: x[1], reverse=True)
        
        return [kw[0] for kw in sorted_keywords[:top_n]]
    
    def classify_by_keywords(self, text: str) -> Dict[str, Any]:
        """
        Classifica o texto baseado em palavras-chave (para fallback).
        
        Args:
            text: Texto a classificar
            
        Returns:
            Dicion√°rio com resultado da classifica√ß√£o
        """
        text_lower = text.lower()
        
        # Contagem de palavras produtivas
        productive_count = 0
        for keyword in PRODUCTIVE_KEYWORDS:
            if keyword in text_lower:
                productive_count += 1
                # Palavras muito importantes t√™m peso duplo
                if keyword in ['problema', 'erro', 'urgente', 'suporte', 'quebr']:
                    productive_count += 1
        
        # Contagem de palavras improdutivas
        unproductive_count = 0
        for keyword in UNPRODUCTIVE_KEYWORDS:
            if keyword in text_lower:
                unproductive_count += 1
        
        # L√≥gica de decis√£o melhorada
        if productive_count > 0 and productive_count >= unproductive_count:
            category = "Produtivo"
            confidence = min(0.8, 0.5 + (productive_count * 0.1))
            reason = f"Detectadas {productive_count} palavras-chave produtivas"
        elif unproductive_count > productive_count:
            category = "Improdutivo"
            confidence = min(0.8, 0.5 + (unproductive_count * 0.1))
            reason = f"Detectadas {unproductive_count} palavras-chave improdutivas"
        else:
            # Empate ou nenhuma palavra-chave - tende para produtivo (mais seguro)
            category = "Produtivo"
            confidence = 0.5
            reason = "Nenhuma palavra-chave clara detectada - classifica√ß√£o padr√£o para produtivo"
        
        return {
            'category': category,
            'confidence': confidence,
            'reason': reason,
            'productive_count': productive_count,
            'unproductive_count': unproductive_count
        }
    
    def preprocess(self, text: str) -> Dict[str, Any]:
        """
        Pipeline completo de pr√©-processamento NLP.
        
        Args:
            text: Texto original
            
        Returns:
            Dicion√°rio com resultados do processamento
        """
        try:
            # 1. Normaliza√ß√£o
            normalized = self.normalize_text(text)
            
            # 2. Tokeniza√ß√£o
            tokens = self.tokenize(normalized)
            
            # 3. Remo√ß√£o de stop words
            tokens_clean = self.remove_stop_words(tokens) if self.remove_stopwords else tokens
            
            # 4. Stemming
            tokens_stemmed = self.stem_tokens(tokens_clean)
            
            # 5. Extra√ß√£o de palavras-chave
            keywords = self.extract_keywords(text)
            
            # 6. Classifica√ß√£o por palavras-chave
            classification = self.classify_by_keywords(text)
            
            # 7. Estat√≠sticas
            statistics = {
                'original_length': len(text),
                'token_count': len(tokens),
                'unique_tokens': len(set(tokens)),
                'tokens_after_stopwords': len(tokens_clean),
                'tokens_after_stemming': len(tokens_stemmed),
                'keyword_count': len(keywords),
                'productive_keywords_found': classification['productive_count'],
                'unproductive_keywords_found': classification['unproductive_count']
            }
            
            logger.info(f"Texto processado: {statistics['token_count']} tokens, "
                       f"{statistics['keyword_count']} keywords, "
                       f"Classifica√ß√£o: {classification['category']} "
                       f"(confian√ßa: {classification['confidence']})")
            
            return {
                'original': text,
                'normalized': normalized,
                'tokens': tokens,
                'tokens_clean': tokens_clean,
                'tokens_stemmed': tokens_stemmed,
                'keywords': keywords,
                'statistics': statistics,
                'classification': classification,
                'processed_text': ' '.join(tokens_stemmed)
            }
            
        except Exception as e:
            logger.error(f"Erro no pr√©-processamento NLP: {str(e)}")

            # Garante que sempre teremos uma string para o fallback
            text_str = text if isinstance(text, str) else str(text)

            tokens_fallback = text_str.split()
            classification_fallback = {
                'category': 'Produtivo',  # Padr√£o seguro
                'confidence': 0.5,
                'reason': 'Erro no processamento - classifica√ß√£o padr√£o',
                'productive_count': 0,
                'unproductive_count': 0
            }
            
            return {
                'original': text_str,
                'normalized': text_str.lower(),
                'tokens': tokens_fallback,
                'tokens_clean': tokens_fallback,
                'tokens_stemmed': tokens_fallback,
                'keywords': [],
                'statistics': {
                    'original_length': len(text_str),
                    'token_count': len(tokens_fallback),
                    'unique_tokens': len(set(tokens_fallback)),
                    'tokens_after_stopwords': len(tokens_fallback),
                    'tokens_after_stemming': len(tokens_fallback),
                    'keyword_count': 0,
                    'productive_keywords_found': 0,
                    'unproductive_keywords_found': 0
                },
                'classification': classification_fallback,
                'processed_text': text_str,
                'error': str(e)
            }


def clean_email_text(text: str) -> str:
    """
    Limpeza espec√≠fica para emails (remove assinaturas, disclaimers, etc).
    Tamb√©m normaliza para min√∫sculas e remove pontua√ß√£o.
    """
    # Converte para min√∫sculas
    text = text.lower()

    # Remove m√∫ltiplas quebras de linha
    text = re.sub(r'\n{3,}', '\n\n', text)

    # Remove m√∫ltiplos espa√ßos
    text = re.sub(r' {2,}', ' ', text)

    # Remove linhas com apenas s√≠mbolos
    text = re.sub(r'^[^\w\s]+$', '', text, flags=re.MULTILINE)

    # Remove URLs
    text = re.sub(r'http[s]?://\S+', '', text)

    # Remove emails
    text = re.sub(r'\S+@\S+', '', text)

    # Remove n√∫meros de telefone
    text = re.sub(r'\(?\d{2}\)?\s?\d{4,5}-?\d{4}', '', text)

    # Remove pontua√ß√£o geral
    text = re.sub(r'[^\w\s]', '', text)

    # Normaliza espa√ßos extras
    text = re.sub(r'\s+', ' ', text)

    return text.strip()


def process_email_text(text: str) -> Dict[str, Any]:
    """
    Fun√ß√£o helper para processar texto de email rapidamente.
    
    Args:
        text: Texto do email
        
    Returns:
        Dicion√°rio com resultado do processamento
    """
    # Limpa o email primeiro
    cleaned = clean_email_text(text)
    
    # Processa com NLP
    processor = TextProcessor(remove_stopwords=True, apply_stemming=True)
    result = processor.preprocess(cleaned)
    
    return result


# Exemplo de uso
if __name__ == "__main__":
    # Teste com email exemplo
    email_exemplo = """
    Prezados,
    
    Estou entrando em contato para solicitar urgentemente o status da minha 
    requisi√ß√£o #12345 que foi aberta na √∫ltima segunda-feira. O sistema est√° 
    apresentando erros cr√≠ticos e preciso de uma solu√ß√£o imediata.
    
    Agrade√ßo antecipadamente pela aten√ß√£o.
    
    Atenciosamente,
    Jo√£o Silva
    joao.silva@empresa.com
    (11) 98765-4321
    """
    
    result = process_email_text(email_exemplo)
    
    print("="*50)
    print("PROCESSAMENTO NLP - EXEMPLO")
    print("="*50)
    print(f"\nüìä Estat√≠sticas:")
    for key, value in result['statistics'].items():
        print(f"  ‚Ä¢ {key}: {value}")
    
    print(f"\nüîë Palavras-chave extra√≠das:")
    print(f"  {', '.join(result['keywords'][:5])}")
    
    print(f"\nüéØ Classifica√ß√£o por palavras-chave:")
    classification = result['classification']
    print(f"  ‚Ä¢ Categoria: {classification['category']}")
    print(f"  ‚Ä¢ Confian√ßa: {classification['confidence']:.2f}")
    print(f"  ‚Ä¢ Motivo: {classification['reason']}")
    
    print(f"\n‚ú® Texto processado (stemming + sem stopwords):")
    print(f"  {result['processed_text'][:100]}...")